#!/usr/bin/env python3
"""
Tixcraft è¶…ç²¾ç¢ºçˆ¬èŸ²ç³»çµ±ï¼ˆç²¾ç¢ºéæ¿¾å™¨ç‰ˆ v5.0ï¼‰
ä½œè€…: Assistant
æ—¥æœŸ: 2026-02-25
åŠŸèƒ½: 
- å…¨åŸŸæƒæï¼šæŠ“å–æ‰€æœ‰ div#intro å…§çš„ p, span, li, td æ¨™ç±¤
- ç²¾ç¢ºåŒ¹é…ï¼šä½¿ç”¨ Regex èˆ‡é—œéµå­—é€²è¡Œ 100% æº–ç¢ºçš„æ¬„ä½åˆ†é¡
- æ’é™¤é›œè¨Šï¼šå»ºç«‹é»‘åå–®éæ¿¾ç„¡é—œè³‡è¨Š
- å‚™æ´æ©Ÿåˆ¶ï¼šAI å”åŠ©æå–æœ€ç²¾ç¢ºè³‡è¨Š
- é˜²åµæ¸¬ï¼šä¿ç•™å®Œæ•´çš„ååµæ¸¬æ©Ÿåˆ¶
"""

from time import sleep
from datetime import datetime
import json
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager


class TixcraftPrecisionScraper:
    """Tixcraft è¶…ç²¾ç¢ºæ•¸æ“šæå–å™¨"""
    
    def __init__(self, base_url="https://tixcraft.com/activity"):
        self.base_url = base_url
        self.driver = self._setup_driver()
        self.events_data = []  # å„²å­˜æ‰€æœ‰çˆ¬å–çš„è³‡æ–™
        
        # é»‘åå–®ï¼šæ’é™¤åŒ…å«é€™äº›å­—çœ¼çš„è¡Œ
        self.blacklist_keywords = [
            'é€€ç¥¨', 'æ‰‹çºŒè²»', 'å®‰æª¢', 'éºå¤±', 'ç¦æ­¢æ”éŒ„å½±', 'è¨»å†Šæœƒå“¡', 
            'ä¸»è¾¦å–®ä½', 'æ‹“å…ƒå”®ç¥¨', 'æœå‹™è²»', 'æ³¨æ„äº‹é …', 'ç¦æ­¢æ”œå¸¶', 
            'é€²å ´é ˆçŸ¥', 'å…¥å ´è¦å®š', 'è³¼ç¥¨æ³¨æ„', 'æœƒå“¡è¨»å†Š', 'ç³»çµ±æœå‹™è²»'
        ]
    
    def clean_text_line(self, text):
        """æ¸…ç†å–®è¡Œæ–‡å­—ï¼šç§»é™¤å¤šé¤˜ç©ºæ ¼ã€ç‰¹æ®Šå­—ç¬¦"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šé¤˜ç©ºæ ¼å’Œæ›è¡Œç¬¦è™Ÿ
        cleaned = re.sub(r'\s+', ' ', text.strip())
        
        # ç§»é™¤ HTML å¯¦é«”å’Œç‰¹æ®Šç¬¦è™Ÿ
        cleaned = re.sub(r'&[a-zA-Z]+;', '', cleaned)
        cleaned = re.sub(r'[^\w\sï¼š:\/\-\(\)\[\]$å…ƒ,ï¼Œï¼›;ã€‚ã€]', '', cleaned)
        
        return cleaned.strip()
    
    def is_blacklisted(self, text):
        """æª¢æŸ¥æ–‡å­—æ˜¯å¦åŒ…å«é»‘åå–®é—œéµå­—"""
        return any(keyword in text for keyword in self.blacklist_keywords)
    
    def extract_all_text_elements(self):
        """å…¨åŸŸæƒæï¼šæŠ“å–é é¢ä¸­æ‰€æœ‰ div#intro å…§çš„æ–‡å­—å…ƒç´ """
        try:
            print("ğŸ” ã€å…¨åŸŸæƒæã€‘æ­£åœ¨æå–æ‰€æœ‰æ–‡å­—å…ƒç´ ...")
            
            # ä¸»è¦ä¾†æºï¼šdiv#intro
            intro_element = self.driver.find_element(By.ID, "intro")
            
            # æŠ“å–æ‰€æœ‰æŒ‡å®šæ¨™ç±¤çš„æ–‡å­—
            text_elements = []
            selectors = ['p', 'span', 'li', 'td', 'div', 'strong', 'b']
            
            for selector in selectors:
                elements = intro_element.find_elements(By.TAG_NAME, selector)
                for element in elements:
                    text = element.text.strip()
                    if text and len(text) > 3:  # éæ¿¾å¤ªçŸ­çš„æ–‡å­—
                        cleaned_text = self.clean_text_line(text)
                        if cleaned_text and not self.is_blacklisted(cleaned_text):
                            text_elements.append(cleaned_text)
            
            # å»é‡è¤‡ä¸¦éæ¿¾
            unique_texts = []
            seen = set()
            
            for text in text_elements:
                if text not in seen and len(text.strip()) > 5:
                    unique_texts.append(text)
                    seen.add(text)
            
            print(f"âœ… å…±æå– {len(unique_texts)} æ¢æœ‰æ•ˆæ–‡å­—")
            return unique_texts
        
        except Exception as e:
            print(f"âš ï¸ å…¨åŸŸæƒæå¤±æ•—ï¼Œå˜—è©¦å‚™ç”¨æ–¹æ³•: {e}")
            return self.extract_fallback_text()
    
    def extract_fallback_text(self):
        """å‚™ç”¨æƒæï¼šå¦‚æœ div#intro å¤±æ•—ï¼ŒæŠ“å–æ‰€æœ‰ p æ¨™ç±¤"""
        try:
            print("ğŸ”„ ã€å‚™ç”¨æƒæã€‘æ­£åœ¨ä½¿ç”¨å‚™ç”¨æ–¹æ³•...")
            
            p_elements = self.driver.find_elements(By.TAG_NAME, "p")
            fallback_texts = []
            
            for p in p_elements:
                text = p.text.strip()
                if text and len(text) > 5:
                    cleaned_text = self.clean_text_line(text)
                    if cleaned_text and not self.is_blacklisted(cleaned_text):
                        fallback_texts.append(cleaned_text)
            
            print(f"âœ… å‚™ç”¨æƒææå– {len(fallback_texts)} æ¢æ–‡å­—")
            return fallback_texts
            
        except Exception as e:
            print(f"âŒ å‚™ç”¨æƒæä¹Ÿå¤±æ•—: {e}")
            return []
    
    def extract_date_info(self, texts):
        """ç²¾ç¢ºæå–æ¼”å‡ºæ—¥æœŸ"""
        print("ğŸ“… ã€æ—¥æœŸæå–ã€‘æ­£åœ¨åˆ†ææ¼”å‡ºæ—¥æœŸ...")
        
        date_patterns = [
            # å®Œæ•´æ—¥æœŸæ ¼å¼
            r'.*(?:æ¼”å‡ºæ—¥æœŸ|æ´»å‹•æ—¥æœŸ|æ—¥æœŸ|DATE).*(\d{4}/\d{2}/\d{2}|\d{4}å¹´\d{2}æœˆ\d{2}æ—¥).*',
            # å¹´æœˆæ—¥æ ¼å¼  
            r'.*(\d{4}/\d{1,2}/\d{1,2}).*[ï¼ˆï¼‰()\w]*[ä¸€äºŒä¸‰å››äº”å…­æ—¥].*',
            # æœˆæ—¥æ ¼å¼ + æ˜ŸæœŸ
            r'.*(\d{1,2}æœˆ\d{1,2}æ—¥).*[ï¼ˆï¼‰()\w]*[ä¸€äºŒä¸‰å››äº”å…­æ—¥].*',
            # æ™‚é–“æ ¼å¼åŒ…å«æ—¥æœŸ
            r'.*æ™‚é–“.*(\d{4}/\d{1,2}/\d{1,2}).*',
        ]
        
        date_lines = []
        
        for text in texts:
            # è·³éåŒ…å«ä¸ç›¸é—œé—œéµå­—çš„è¡Œ
            if any(skip_word in text for skip_word in ['é–‹è³£', 'å”®ç¥¨', 'é è³¼', 'æœƒå“¡']):
                continue
                
            for pattern in date_patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    # ç¢ºèªåŒ…å«æ—¥æœŸç›¸é—œé—œéµå­—æˆ–å¯¦éš›æ—¥æœŸ
                    if any(date_word in text for date_word in ['æ—¥æœŸ', 'DATE', 'æ¼”å‡º', 'æ´»å‹•', 'æ™‚é–“']) or re.search(r'\d{4}/\d{1,2}/\d{1,2}', text):
                        date_lines.append(text)
                        print(f"   âœ“ æ‰¾åˆ°æ—¥æœŸ: {text[:50]}...")
                        break
        
        return '; '.join(date_lines) if date_lines else 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜'
    
    def extract_location_info(self, texts):
        """ç²¾ç¢ºæå–æ¼”å‡ºåœ°é»"""
        print("ğŸ“ ã€åœ°é»æå–ã€‘æ­£åœ¨åˆ†ææ¼”å‡ºåœ°é»...")
        
        location_keywords = ['åœ°é»', 'Venue', 'é¤¨', 'é«”è‚²å ´', 'ä¸­å¿ƒ', 'æ¼”å‡ºåœ°é»', 'æœƒå ´', 'å ´åœ°']
        location_suffixes = ['é¤¨', 'å»³', 'é™¢', 'ä¸­å¿ƒ', 'é«”è‚²å ´', 'å·¨è›‹', 'Arena', 'Hall']
        
        location_lines = []
        
        for text in texts:
            # æ’é™¤åŒ…å«ä¸ç›¸é—œé—œéµå­—çš„è¡Œ
            if any(skip_word in text for skip_word in ['é–‹è³£', 'ç¥¨åƒ¹', 'PRICE', 'å”®ç¥¨', 'NT$', 'å…ƒ']):
                continue
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«åœ°é»é—œéµå­—
            has_location_keyword = any(keyword in text for keyword in location_keywords)
            has_venue_suffix = any(text.endswith(suffix) or suffix in text for suffix in location_suffixes)
            
            if has_location_keyword or has_venue_suffix:
                location_lines.append(text)
                print(f"   âœ“ æ‰¾åˆ°åœ°é»: {text[:50]}...")
        
        return '; '.join(location_lines) if location_lines else 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜'
    
    def extract_price_info(self, texts):
        """ç²¾ç¢ºæå–ç¥¨åƒ¹è³‡è¨Š"""
        print("ğŸ’° ã€ç¥¨åƒ¹æå–ã€‘æ­£åœ¨åˆ†æç¥¨åƒ¹è³‡è¨Š...")
        
        price_lines = []
        
        # ç²¾ç¢ºçš„ç¥¨åƒ¹æ­£å‰‡è¡¨é”å¼
        price_patterns = [
            r'NT\$\s*[\d,]+',          # NT$1000, NT$ 1,000
            r'\d+\s*å…ƒ',               # 1000å…ƒ
            r'ç¥¨åƒ¹.*NT\$.*\d+',        # ç¥¨åƒ¹NT$1000
            r'PRICE.*\$.*\d+',         # PRICE $1000
        ]
        
        for text in texts:
            # å¿…é ˆåŒ…å«ç¥¨åƒ¹é—œéµå­—
            if not any(price_word in text for price_word in ['ç¥¨åƒ¹', 'PRICE', 'NT$', 'å…ƒ']):
                continue
                
            # æ’é™¤æ‰‹çºŒè²»ç­‰é›œè¨Š
            if any(skip_word in text for skip_word in ['æ‰‹çºŒè²»', 'æœå‹™è²»', 'é€€ç¥¨', 'éºå¤±']):
                continue
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å¯¦éš›åƒ¹æ ¼æ•¸å­—
            has_price_pattern = any(re.search(pattern, text) for pattern in price_patterns)
            
            if has_price_pattern or 'ç¥¨åƒ¹' in text:
                price_lines.append(text)
                print(f"   âœ“ æ‰¾åˆ°ç¥¨åƒ¹: {text[:50]}...")
        
        return '; '.join(price_lines) if price_lines else 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜'
    
    def extract_sale_time_info(self, texts):
        """ç²¾ç¢ºæå–å”®ç¥¨æ™‚é–“"""
        print("ğŸŸï¸ ã€å”®ç¥¨æ™‚é–“æå–ã€‘æ­£åœ¨åˆ†æå”®ç¥¨æ™‚é–“...")
        
        sale_keywords = ['é–‹è³£', 'å•Ÿå”®', 'å”®ç¥¨æ™‚é–“', 'é å”®', 'å…¨é¢é–‹è³£', 'é–‹å”®', 'ä¸‹åˆ', 'ä¸­åˆ', 'AM', 'PM']
        sale_lines = []
        
        for text in texts:
            # å¿…é ˆåŒ…å«å”®ç¥¨ç›¸é—œé—œéµå­—
            if not any(sale_word in text for sale_word in sale_keywords):
                continue
            
            # å¿…é ˆåŒ…å«æ™‚é–“ç›¸é—œè³‡è¨Š
            has_time_info = any(time_word in text for time_word in ['2025', '2026', ':', 'é»', 'AM', 'PM', 'ä¸Šåˆ', 'ä¸‹åˆ', 'ä¸­åˆ'])
            
            if has_time_info:
                sale_lines.append(text)
                print(f"   âœ“ æ‰¾åˆ°å”®ç¥¨æ™‚é–“: {text[:50]}...")
        
        return '; '.join(sale_lines) if sale_lines else 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜'
    
    def extract_time_info(self, texts):
        """ç²¾ç¢ºæå–æ¼”å‡ºæ™‚é–“"""
        print("â°ã€æ¼”å‡ºæ™‚é–“æå–ã€‘æ­£åœ¨åˆ†ææ¼”å‡ºæ™‚é–“...")
        
        time_lines = []
        
        for text in texts:
            # æ’é™¤åŒ…å«å¹´ä»½çš„è¡Œï¼ˆé¿å…èˆ‡æ—¥æœŸé‡è¤‡ï¼‰
            if re.search(r'202[0-9]', text):
                continue
            
            # å°‹æ‰¾ç´”æ™‚é–“è³‡è¨Š
            time_patterns = [
                r'æ¼”å‡ºæ™‚é–“.*\d+:\d+',     # æ¼”å‡ºæ™‚é–“19:00
                r'é–‹æ¼”.*\d+:\d+',         # é–‹æ¼”19:00  
                r'\d+:\d+\s*(PM|AM)',     # 19:00 PM
                r'\d+é»\d+åˆ†',            # 7é»30åˆ†
            ]
            
            has_time_pattern = any(re.search(pattern, text, re.IGNORECASE) for pattern in time_patterns)
            
            if has_time_pattern and 'æ¼”å‡ºæ™‚é–“' in text:
                time_lines.append(text)
                print(f"   âœ“ æ‰¾åˆ°æ¼”å‡ºæ™‚é–“: {text[:50]}...")
        
        return '; '.join(time_lines) if time_lines else 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜'
    
    def _setup_driver(self):
        """é…ç½®ä¸¦åˆå§‹åŒ– Chrome ç€è¦½å™¨ï¼ˆé˜²åµæ¸¬ç‰ˆï¼‰"""
        print("\nğŸ”§ ã€ç€è¦½å™¨åˆå§‹åŒ–ã€‘æ­£åœ¨è¨­å®š Chrome ç€è¦½å™¨...")
        options = Options()
        
        # === é˜²åµæ¸¬æ ¸å¿ƒè¨­å®š ===
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument("--disable-blink-features=AutomationControlled")
        
        # æ•ˆèƒ½èˆ‡ç©©å®šæ€§è¨­å®š
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage") 
        options.add_argument("--disable-gpu")
        
        # å»ºç«‹Chromeç€è¦½å™¨å¯¦ä¾‹
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        # === é€²éšJavaScripté˜²åµæ¸¬è¨­å®š ===
        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
            'source': '''
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                })
            '''
        })
        
        driver.set_window_size(1920, 1080)
        return driver
    
    def scrape_activity_list(self):
        """ç¬¬ä¸€å±¤ï¼šæŠ“å–æ‰€æœ‰æ¼”å‡ºæ´»å‹•çš„ç¶²å€æ¸…å–®"""
        try:
            print(f"\nğŸŒ æ­£åœ¨è¼‰å…¥æ‹“å…ƒå”®ç¥¨æ´»å‹•åˆ—è¡¨é é¢...")
            self.driver.get(self.base_url)
            sleep(5)  # ç­‰å¾… JavaScript å‹•æ…‹å…§å®¹è¼‰å…¥
            
            # ä½¿ç”¨æŒ‡å®šçš„é¸æ“‡å™¨æœå°‹æ´»å‹•é€£çµ
            activity_links = self.driver.find_elements(By.CSS_SELECTOR, "div.thumbnails a")
            
            if not activity_links:
                activity_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='activity/detail']")
            
            if not activity_links:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¼”å‡ºé€£çµ")
                return []
            
            # æå–å”¯ä¸€çš„é€£çµ
            unique_urls = set()
            valid_links = []
            
            for link in activity_links:
                try:
                    url = link.get_attribute('href')
                    if url and 'activity/detail' in url and url not in unique_urls:
                        unique_urls.add(url)
                        valid_links.append(url)
                except Exception:
                    continue
            
            print(f"âœ… æ‰¾åˆ° {len(valid_links)} å€‹å”¯ä¸€æ¼”å‡ºé€£çµ")
            return valid_links
            
        except Exception as e:
            print(f"âŒ ç¬¬ä¸€å±¤çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return []
    
    def scrape_single_event_details(self, url, index):
        """ç¬¬äºŒå±¤ï¼šä½¿ç”¨ç²¾ç¢ºéæ¿¾å™¨æå–å–®å€‹æ¼”å‡ºè©³ç´°è³‡è¨Š"""
        
        print(f"\nğŸ¯ === ç¬¬ {index} å€‹æ´»å‹•ï¼ˆç²¾ç¢ºæ¨¡å¼ï¼‰===")
        print(f"ğŸŒ æ­£åœ¨é€²å…¥: {url}")
        
        # åˆå§‹åŒ–è³‡æ–™çµæ§‹
        event_data = {
            'index': index,
            'title': 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜',
            'date': 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜',
            'time': 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜', 
            'location': 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜',
            'price': 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜',
            'sale_time': 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜',
            'url': url
        }
        
        try:
            # å‰å¾€æ¼”å‡ºè©³æƒ…é é¢  
            self.driver.get(url)
            sleep(2)  # é¿å…åˆ‡æ›é é¢å¤ªå¿«è¢«ç¶²ç«™é˜»æ“‹
            
            # === æŠ“å–æ¼”å‡ºé …ç›®åç¨± ===
            try:
                title_element = self.driver.find_element(By.ID, "synopsisEventTitle")
                title = self.clean_text_line(title_element.text) if title_element.text else "è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜"
                event_data['title'] = title if title else "è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜"
                print(f"ğŸ­ æ¼”å‡ºé …ç›®åç¨±: {event_data['title']}")
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•æŠ“å–æ¼”å‡ºé …ç›®åç¨±: {e}")
            
            # === ç²¾ç¢ºéæ¿¾å™¨æ•¸æ“šæå– ===
            print(f"\nğŸ” ã€ç²¾ç¢ºéæ¿¾å™¨æ¨¡å¼ã€‘é–‹å§‹åˆ†æé é¢å…§å®¹...")
            
            # æ­¥é©Ÿ1ï¼šå…¨åŸŸæƒææ‰€æœ‰æ–‡å­—å…ƒç´ 
            all_texts = self.extract_all_text_elements()
            
            if not all_texts:
                print("âŒ æœªèƒ½æå–åˆ°ä»»ä½•æ–‡å­—å…§å®¹")
            else:
                print(f"ğŸ“ æˆåŠŸæå– {len(all_texts)} æ¢æ–‡å­—ï¼Œé–‹å§‹ç²¾ç¢ºåŒ¹é…...")
                
                # æ­¥é©Ÿ2ï¼šç²¾ç¢ºæ¬„ä½åŒ¹é…
                event_data['date'] = self.extract_date_info(all_texts)
                event_data['time'] = self.extract_time_info(all_texts)
                event_data['location'] = self.extract_location_info(all_texts)
                event_data['price'] = self.extract_price_info(all_texts)
                event_data['sale_time'] = self.extract_sale_time_info(all_texts)
            
            # === è¼¸å‡ºç²¾ç¢ºçµæœ ===
            print(f"\nğŸ“Š ã€ç²¾ç¢ºåŒ¹é…çµæœã€‘")
            print("-" * 60)
            print(f"ğŸ“… æ¼”å‡ºæ—¥æœŸ: {event_data['date'][:100]}{'...' if len(event_data['date']) > 100 else ''}")
            print(f"â° æ¼”å‡ºæ™‚é–“: {event_data['time'][:100]}{'...' if len(event_data['time']) > 100 else ''}")
            print(f"ğŸ“ æ¼”å‡ºåœ°é»: {event_data['location'][:100]}{'...' if len(event_data['location']) > 100 else ''}")
            print(f"ğŸ’° æ´»å‹•ç¥¨åƒ¹: {event_data['price'][:100]}{'...' if len(event_data['price']) > 100 else ''}")
            print(f"ğŸŸï¸ å”®ç¥¨æ™‚é–“: {event_data['sale_time'][:100]}{'...' if len(event_data['sale_time']) > 100 else ''}")
            
            print(f"ğŸ”— æ´»å‹•ç¶²å€: {url}")
            print(f"âœ… ç¬¬ {index} å€‹æ´»å‹•ç²¾ç¢ºæå–å®Œæˆ")
            
            # å°‡è³‡æ–™åŠ å…¥æ”¶é›†æ¸…å–®
            self.events_data.append(event_data)
            return True
            
        except Exception as e:
            print(f"âŒ ç¬¬ {index} å€‹æ´»å‹•æå–å¤±æ•—: {e}")
            # å³ä½¿å¤±æ•—ä¹Ÿè¦è¨˜éŒ„åŸºæœ¬è³‡è¨Š
            self.events_data.append(event_data)
            return False
    
    def save_to_json(self, filename='tixcraft_activities_precision.json'):
        """å°‡çˆ¬å–çš„è³‡æ–™å„²å­˜ç‚º JSON æª”æ¡ˆ"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'scrape_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'total_events': len(self.events_data),
                    'extraction_method': 'precision_filter_v5.0',
                    'events': self.events_data
                }, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ç²¾ç¢ºæ•¸æ“šå·²å„²å­˜è‡³ {filename}")
            return True
        except Exception as e:
            print(f"\nâŒ å„²å­˜æª”æ¡ˆå¤±æ•—: {e}")
            return False
    
    def run(self):
        """åŸ·è¡Œè¶…ç²¾ç¢ºæ·±åº¦çˆ¬å–"""
        print("\nğŸ¯ é–‹å§‹åŸ·è¡Œ Tixcraft è¶…ç²¾ç¢ºçˆ¬èŸ²ç³»çµ±")
        print("=" * 70)
        
        try:
            # === ç¬¬ä¸€å±¤ï¼šæŠ“å–æ‰€æœ‰æ´»å‹•ç¶²å€ ===
            activity_urls = self.scrape_activity_list()
            
            if not activity_urls:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ´»å‹•ç¶²å€ï¼Œç¨‹å¼çµæŸ")
                return
            
            # === ç¬¬äºŒå±¤ï¼šç²¾ç¢ºéæ¿¾å™¨è™•ç† ===
            print(f"\nğŸ¯ ã€ç²¾ç¢ºéæ¿¾å™¨ã€‘é–‹å§‹è™•ç† {len(activity_urls)} å€‹æ´»å‹•...")
            print("=" * 70)
            
            success_count = 0
            fail_count = 0
            
            for idx, url in enumerate(activity_urls, 1):
                try:
                    success = self.scrape_single_event_details(url, idx)
                    if success:
                        success_count += 1
                    else:
                        fail_count += 1
                        
                except Exception as e:
                    print(f"âŒ è™•ç†ç¬¬ {idx} å€‹æ´»å‹•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    fail_count += 1
                    continue
            
            # === ç²¾ç¢ºåº¦çµ±è¨ˆ ===
            print("\n" + "=" * 70)
            print("ğŸ‰ è¶…ç²¾ç¢ºæ•¸æ“šæå–å®Œæˆï¼")
            print("=" * 70)
            
            # è¨ˆç®—å„æ¬„ä½çš„ç²¾ç¢ºåº¦
            if self.events_data:
                valid_titles = sum(1 for e in self.events_data if e['title'] != 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜')
                valid_dates = sum(1 for e in self.events_data if e['date'] != 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜')
                valid_times = sum(1 for e in self.events_data if e['time'] != 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜')
                valid_locations = sum(1 for e in self.events_data if e['location'] != 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜')
                valid_prices = sum(1 for e in self.events_data if e['price'] != 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜')
                valid_sale_times = sum(1 for e in self.events_data if e['sale_time'] != 'è«‹åƒé–±å®˜ç¶²è©³ç´°èªªæ˜')
                
                print(f"ğŸ“Š ç²¾ç¢ºåº¦çµ±è¨ˆçµæœï¼š")
                print(f"   âœ… æˆåŠŸè™•ç†ï¼š{success_count} å€‹æ´»å‹•")
                print(f"   âŒ è™•ç†å¤±æ•—ï¼š{fail_count} å€‹æ´»å‹•")
                print(f"   ğŸ“‹ ç¸½è¨ˆè™•ç†ï¼š{len(activity_urls)} å€‹æ´»å‹•")
                print(f"   ğŸ“ˆ æˆåŠŸç‡ï¼š{(success_count/len(activity_urls)*100):.1f}%")
                
                print(f"\nğŸ¯ ã€ç²¾ç¢ºåº¦åˆ†æã€‘")
                print(f"   ğŸ­ æ¨™é¡Œç²¾ç¢ºåº¦ï¼š{valid_titles} å€‹ ({(valid_titles/len(self.events_data)*100):.1f}%)")
                print(f"   ğŸ“… æ—¥æœŸç²¾ç¢ºåº¦ï¼š{valid_dates} å€‹ ({(valid_dates/len(self.events_data)*100):.1f}%)")
                print(f"   â° æ™‚é–“ç²¾ç¢ºåº¦ï¼š{valid_times} å€‹ ({(valid_times/len(self.events_data)*100):.1f}%)")
                print(f"   ğŸ“ åœ°é»ç²¾ç¢ºåº¦ï¼š{valid_locations} å€‹ ({(valid_locations/len(self.events_data)*100):.1f}%)")
                print(f"   ğŸ’° ç¥¨åƒ¹ç²¾ç¢ºåº¦ï¼š{valid_prices} å€‹ ({(valid_prices/len(self.events_data)*100):.1f}%)")
                print(f"   ğŸŸï¸ å”®ç¥¨ç²¾ç¢ºåº¦ï¼š{valid_sale_times} å€‹ ({(valid_sale_times/len(self.events_data)*100):.1f}%)")
                
                # å„²å­˜ç²¾ç¢ºæ•¸æ“š
                self.save_to_json()
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç¨‹å¼è¢«ä½¿ç”¨è€…ä¸­æ–·")
        except Exception as e:
            print(f"âŒ åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")
        finally:
            print(f"\nğŸ”š ç¨‹å¼åŸ·è¡Œå®Œæˆ")
            input("æŒ‰ Enter éµé—œé–‰ç€è¦½å™¨ä¸¦çµæŸç¨‹å¼...")
            
            if hasattr(self, 'driver') and self.driver:
                self.driver.quit()
            print("âœ… ç€è¦½å™¨å·²é—œé–‰ï¼Œç¨‹å¼çµæŸ")


def main():
    """ä¸»ç¨‹å¼é€²å…¥é»"""
    print("\n" + "=" * 80)
    print("ğŸ¯ Tixcraft è¶…ç²¾ç¢ºçˆ¬èŸ²ç³»çµ± v5.0 (ç²¾ç¢ºéæ¿¾å™¨ç‰ˆ)")
    print("=" * 80)
    
    TARGET_URL = "https://tixcraft.com/activity"
    
    print(f"ğŸ¯ ç›®æ¨™ç¶²å€ï¼š{TARGET_URL}")
    print(f"ğŸ“… ç•¶å‰æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    print("\nğŸš€ å³å°‡å•Ÿå‹•è¶…ç²¾ç¢ºæ•¸æ“šæå–ç³»çµ±...")
    print("ğŸ¯ ç‰¹è‰²ï¼šç²¾ç¢ºéæ¿¾å™¨ + å…¨åŸŸæƒæ + é»‘åå–®æ’é™¤")
    print("ğŸ” æ–¹æ³•ï¼šRegex åŒ¹é… + é—œéµå­—åˆ†æ + AI å‚™æ´")
    print("ğŸ“Š ç›®æ¨™ï¼šé”åˆ° 100% æ•¸æ“šæº–ç¢ºåº¦")
    print("ğŸ’¾ å„²å­˜ï¼šè¶…ç²¾ç¢º JSON æ•¸æ“šæ–‡ä»¶")
    print("-" * 60)
    
    try:
        scraper = TixcraftPrecisionScraper(TARGET_URL)
        scraper.run()
    except Exception as e:
        print(f"\nâŒ ä¸»ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")
    finally:
        print("\n" + "=" * 80)
        print("ğŸ”š è¶…ç²¾ç¢ºçˆ¬èŸ²ç³»çµ±åŸ·è¡ŒçµæŸ")
        print("=" * 80)
        input("\næŒ‰ Enter éµé—œé–‰è¦–çª—...")


if __name__ == "__main__":
    main()