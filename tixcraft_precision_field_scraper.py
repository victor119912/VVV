import json
import re
import time
import random
import logging
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

class TixcraftPrecisionFieldScraper:
    def __init__(self):
        self.setup_logging()
        self.driver = None
        
    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('tixcraft_precision_field.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def load_existing_data(self, filename='tixcraft_activities.json'):
        """è¼‰å…¥ç¾æœ‰çš„JSONè³‡æ–™"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.logger.info(f"æˆåŠŸè¼‰å…¥ç¾æœ‰è³‡æ–™ï¼š{len(data.get('events', []))} å€‹æ´»å‹•")
                return data
        except FileNotFoundError:
            self.logger.info(f"æª”æ¡ˆ {filename} ä¸å­˜åœ¨ï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆ")
            return None
        except Exception as e:
            self.logger.error(f"è¼‰å…¥ç¾æœ‰è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return None
    
    def merge_data(self, existing_data, new_events):
        """åˆä½µæ–°èˆŠè³‡æ–™ï¼Œä»¥URLä½œç‚ºå”¯ä¸€è­˜åˆ¥ç¬¦å»é‡è¤‡"""
        if not existing_data:
            return new_events
            
        existing_events = existing_data.get('events', [])
        existing_urls = {event['url']: event for event in existing_events}
        
        # æ›´æ–°æˆ–æ–°å¢äº‹ä»¶
        updated_count = 0
        new_count = 0
        
        for new_event in new_events:
            url = new_event['url']
            if url in existing_urls:
                # æ›´æ–°ç¾æœ‰äº‹ä»¶ï¼ˆä¿ç•™ç´¢å¼•ä½†æ›´æ–°å…¶ä»–è³‡æ–™ï¼‰
                old_index = existing_urls[url].get('index', new_event['index'])
                new_event['index'] = old_index
                existing_urls[url] = new_event
                updated_count += 1
                self.logger.debug(f"æ›´æ–°æ´»å‹•ï¼š{new_event['title']}")
            else:
                # æ–°å¢äº‹ä»¶
                existing_urls[url] = new_event
                new_count += 1
                self.logger.debug(f"æ–°å¢æ´»å‹•ï¼š{new_event['title']}")
        
        # é‡æ–°æ’åºä¸¦åˆ†é…ç´¢å¼•
        merged_events = list(existing_urls.values())
        for i, event in enumerate(merged_events, 1):
            event['index'] = i
            
        self.logger.info(f"è³‡æ–™åˆä½µå®Œæˆï¼šæ›´æ–° {updated_count} å€‹æ´»å‹•ï¼Œæ–°å¢ {new_count} å€‹æ´»å‹•ï¼Œç¸½è¨ˆ {len(merged_events)} å€‹æ´»å‹•")
        return merged_events
        
    def get_random_user_agent(self):
        """éš¨æ©Ÿé¸æ“‡çœŸå¯¦çš„ User-Agent å­—ä¸²"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        ]
        return random.choice(user_agents)
        
    def setup_driver(self):
        """è¨­ç½®Chromeç€è¦½å™¨é¸é … - å¼·åŒ–ç‰ˆååµæ¸¬åŠŸèƒ½"""
        chrome_options = Options()
        
        # é—œé–‰ Headless æ¨¡å¼ä»¥ä¾¿æ‰‹å‹•é©—è­‰
        # chrome_options.add_argument('--headless')
        
        # åŸºæœ¬ç©©å®šæ€§è¨­å®š
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1366,768')  # å¸¸è¦‹è§£æåº¦
        
        # å¼·åŒ–ç‰ˆ WebDriver ç‰¹å¾µéš±è—
        chrome_options.add_experimental_option("excludeSwitches", [
            "enable-automation",
            "enable-logging",
            "disable-extensions",
            "test-type",
            "disable-background-timer-throttling",
            "disable-renderer-backgrounding",
            "disable-backgrounding-occluded-windows"
        ])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # éš¨æ©Ÿ User-Agent
        user_agent = self.get_random_user_agent()
        chrome_options.add_argument(f'--user-agent={user_agent}')
        self.logger.info(f"ä½¿ç”¨ User-Agent: {user_agent}")
        
        # é€²éšååµæ¸¬è¨­å®š
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-infobars')
        chrome_options.add_argument('--disable-notifications')
        chrome_options.add_argument('--disable-popup-blocking')
        
        # åŠ é€Ÿè¨­å®š - ç¦ç”¨åœ–ç‰‡å’Œå¤šåª’é«”ï¼ˆä¿ç•™JavaScriptç”¨æ–¼dataLayeræå–ï¼‰
        chrome_options.add_argument('--disable-images')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_experimental_option('prefs', {
            'profile.managed_default_content_settings.images': 2,
            'profile.default_content_setting_values.media_stream': 2,
        })
        
        # æ¨¡æ“¬çœŸå¯¦ç€è¦½è¡Œç‚º
        chrome_options.add_argument('--start-maximized')
        chrome_options.add_experimental_option('prefs', {
            'profile.default_content_setting_values.notifications': 2,
            'profile.default_content_settings.popups': 0
        })
        
        try:
            service = ChromeService(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # åŸ·è¡Œå¼·åŒ–ç‰ˆååµæ¸¬è…³æœ¬
            self.driver.execute_cdp_cmd('Runtime.evaluate', {
                "expression": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            })
            
            # éš±è— Selenium æ¨™è­˜
            self.driver.execute_cdp_cmd('Runtime.evaluate', {
                "expression": "Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})"
            })
            
            self.driver.execute_cdp_cmd('Runtime.evaluate', {
                "expression": "Object.defineProperty(navigator, 'languages', {get: () => ['zh-TW', 'zh', 'en']})"
            })
            
            self.driver.execute_cdp_cmd('Runtime.evaluate', {
                "expression": "window.chrome = { runtime: {} };"
            })
            
            self.logger.info("Chromeç€è¦½å™¨å·²æˆåŠŸå•Ÿå‹•ï¼ˆååµæ¸¬æ¨¡å¼ï¼‰")
        except Exception as e:
            self.logger.error(f"Chromeç€è¦½å™¨å•Ÿå‹•å¤±æ•—ï¼š{e}")
            raise
            
    def clean_text(self, text):
        """ã€æ·±åº¦æ¸…æ´—ç‰ˆã€‘æ¸…æ´—å™¨ï¼šç§»é™¤ç‰¹æ®Šç¬¦è™Ÿã€è£é£¾ç¬¦è™Ÿå’Œé›œè³ª"""
        if not text:
            return ""
            
        # ç§»é™¤è£é£¾ç¬¦è™Ÿå’Œè¡¨æƒ…ç¬¦è™Ÿï¼ˆæ“´å±•ç‰ˆï¼‰
        symbols_to_remove = [
            ';', '&nbsp;', 'â—', 'ğŸ‘‰', 'â€»', 'â˜…', 'â–²', 'â– ', 'â—†', 'ğŸ«', 'ğŸ“', 'ğŸ’', 'â«',
            'â˜†', 'â­', 'ğŸ­', 'â°', 'ğŸ“…', 'ğŸµ', 'ğŸ¶', 'ğŸ¤', 'ğŸ¸', 'ğŸ¹', 'ğŸ¥', 'ğŸº', 'ğŸ·',
            'âœ¦', 'âœ¨', 'ğŸ’«', 'ğŸŒŸ', 'âš¡', 'ğŸ”¥', 'ğŸ’¥', 'âœ…', 'â¤ï¸', 'ğŸ’–', 'ğŸ’', 'ğŸ‰', 'ğŸŠ',
            'ğŸ†', 'ğŸ‘‘', 'ğŸ', 'ğŸˆ', 'ğŸ…', 'ğŸ´', 'ğŸ³ï¸', 'ğŸ“¢', 'ğŸ“£', 'ğŸ“¯', 'ğŸ””', 'ğŸ”•',
            'ã€‹', 'ã€Š', 'ã€‰', 'ã€ˆ', 'ã€', 'ã€‘', 'ã€”', 'ã€•', 'ï¼»', 'ï¼½', 'ï½›', 'ï½'
        ]
        
        for symbol in symbols_to_remove:
            text = text.replace(symbol, '')
        
        # ç§»é™¤è¡¨æƒ…ç¬¦è™Ÿï¼ˆUnicodeç¯„åœï¼‰
        text = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U000026FF\U00002700-\U000027BF]', '', text)
        
        # ç§»é™¤å¤šé¤˜ç©ºç™½ä¸¦æ¸…ç†
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def calculate_similarity(self, text1, text2):
        """è¨ˆç®—å…©å€‹æ–‡å­—çš„ç›¸ä¼¼åº¦ï¼ˆç°¡å–®ç‰ˆæœ¬ï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        # ç§»é™¤ç©ºæ ¼ä¸¦è½‰å°å¯«æ¯”è¼ƒ
        clean1 = re.sub(r'\s+', '', text1.lower())
        clean2 = re.sub(r'\s+', '', text2.lower())
        
        # è¨ˆç®—æœ€é•·å…¬å…±å­åºåˆ—çš„æ¯”ä¾‹
        max_len = max(len(clean1), len(clean2))
        if max_len == 0:
            return 1.0
            
        common_chars = len(set(clean1) & set(clean2))
        return common_chars / max_len
    
    def remove_duplicate_lines(self, lines):
        """ç§»é™¤ç›¸ä¼¼åº¦è¶…é80%çš„é‡è¤‡è¡Œ"""
        if not lines:
            return lines
            
        unique_lines = []
        for current_line in lines:
            is_duplicate = False
            for existing_line in unique_lines:
                if self.calculate_similarity(current_line, existing_line) > 0.8:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_lines.append(current_line)
                
        return unique_lines
    
    def filter_empty_shells(self, lines):
        """ç§»é™¤ç©ºæ®¼æ¨™é¡Œï¼ˆåªæœ‰æ¨™é¡Œæ²’æœ‰å¯¦è³ªå…§å®¹ï¼‰"""
        filtered_lines = []
        empty_shell_patterns = [
            r'^å”®ç¥¨è³‡è¨Šï¼š?\s*$',
            r'^åœ°é»ï¼š?\s*$', 
            r'^ç¥¨åƒ¹ï¼š?\s*$',
            r'^æ™‚é–“ï¼š?\s*$',
            r'^æ—¥æœŸï¼š?\s*$',
            r'^PRICEï¼š?\s*$',
            r'^VENUEï¼š?\s*$',
            r'^TIMEï¼š?\s*$',
            r'^å”®ç¥¨æ–¹å¼ï¼š?\s*$',
            r'^è³¼ç¥¨è³‡è¨Šï¼š?\s*$'
        ]
        
        for line in lines:
            is_empty_shell = any(re.match(pattern, line.strip()) for pattern in empty_shell_patterns)
            if not is_empty_shell and len(line.strip()) > 3:  # è‡³å°‘è¦æœ‰3å€‹å­—ç¬¦
                filtered_lines.append(line)
                
        return filtered_lines
        
    def get_clean_data_from_js(self, driver):
        """
        [æš´åŠ›å¼·åŒ–ç‰ˆ] å‹•æ…‹é‡è©¦ç›£æ§ JavaScript dataLayer æå–
        - æ¯éš”0.5ç§’é‡è©¦ï¼Œæœ€å¤šç­‰å¾…5ç§’
        - æ“´å¤§æœç´¢ç¯„åœï¼šéæ­·æ•´å€‹dataLayeré™£åˆ—
        - åªè¦å«æœ‰ artistNameã€gameCode æˆ– item_name å°±ç«‹åˆ»æå–
        """
        max_wait_time = 5.0  # æœ€å¤šç­‰å¾…5ç§’
        retry_interval = 0.5  # æ¯0.5ç§’é‡è©¦ä¸€æ¬¡
        attempt = 0
        start_time = time.time()
        
        self.logger.info("é–‹å§‹æš´åŠ›ç›£æ§ dataLayerï¼Œæœ€å¤šç­‰å¾…5ç§’...")
        
        while time.time() - start_time < max_wait_time:
            attempt += 1
            try:
                # æš´åŠ›æœç´¢æ•´å€‹ dataLayer é™£åˆ—çš„ JavaScript è…³æœ¬
                js_script = """
                // ç¢ºä¿ dataLayer å­˜åœ¨
                if (typeof dataLayer === 'undefined' || !Array.isArray(dataLayer)) {
                    return null;
                }
                
                // éæ­·æ•´å€‹ dataLayer é™£åˆ—ï¼Œæ“´å¤§æœç´¢ç¯„åœ
                for (let i = 0; i < dataLayer.length; i++) {
                    let item = dataLayer[i];
                    
                    // åªè¦åŒ…å«ä»¥ä¸‹ä»»ä¸€æ¬„ä½å°±ç«‹åˆ»æå–
                    if (item.artistName || item.gameCode || item.item_name) {
                        return {
                            found: true,
                            artistName: item.artistName || item.item_name || '',
                            gameCode: item.gameCode || '',
                            childCategoryName: item.childCategoryName || '',
                            promoter: item.promoter || '',
                            event: item.event || '',
                            venueInfo: item.venueInfo || item.venue || '',
                            location: item.location || '',
                            rawData: item
                        };
                    }
                }
                
                // å¦‚æœæ²’æ‰¾åˆ°ï¼Œè¿”å›æœç´¢ç‹€æ…‹
                return {found: false, dataLayerLength: dataLayer.length};
                """
                
                raw_info = driver.execute_script(js_script)
                
                if raw_info and raw_info.get('found'):
                    # æˆåŠŸæ‰¾åˆ°æ•¸æ“šï¼
                    result = {
                        "title": raw_info.get('artistName', 'æœªæŠ“åˆ°æ¨™é¡Œ'),
                        "category": raw_info.get('childCategoryName', 'æœªæŠ“åˆ°åˆ†é¡'),
                        "game_code": raw_info.get('gameCode', 'N/A'),
                        "promoter": raw_info.get('promoter', 'N/A'),
                        "venue_info": raw_info.get('venueInfo', ''),
                        "location": raw_info.get('location', '')
                    }
                    
                    self.logger.info(f"ğŸ¯ ç¬¬{attempt}æ¬¡å˜—è©¦æˆåŠŸï¼æ‰¾åˆ°dataLayeræ•¸æ“š: {result['title']}")
                    return result
                
                elif raw_info:
                    self.logger.debug(f"ç¬¬{attempt}æ¬¡å˜—è©¦ï¼šdataLayeré•·åº¦={raw_info.get('dataLayerLength', 0)}ï¼Œæœªæ‰¾åˆ°ç›®æ¨™æ•¸æ“š")
                else:
                    self.logger.debug(f"ç¬¬{attempt}æ¬¡å˜—è©¦ï¼šdataLayerä¸å­˜åœ¨æˆ–ç„¡æ³•è¨ªå•")
                
                # å¦‚æœæ²’æ‰¾åˆ°ï¼Œç­‰å¾…å¾Œé‡è©¦
                if time.time() - start_time < max_wait_time:
                    time.sleep(retry_interval)
                    
            except Exception as e:
                self.logger.debug(f"ç¬¬{attempt}æ¬¡JavaScriptåŸ·è¡Œå¤±æ•—: {e}")
                if time.time() - start_time < max_wait_time:
                    time.sleep(retry_interval)
        
        # è¶…æ™‚å¾Œè¿”å›ç©ºçµæœ
        self.logger.warning(f"æš´åŠ›ç›£æ§è¶…æ™‚({max_wait_time}ç§’)ï¼Œå…±å˜—è©¦{attempt}æ¬¡ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆdataLayeræ•¸æ“š")
        return {
            "title": "JSç›£æ§è¶…æ™‚", 
            "category": "æœªæŠ“åˆ°åˆ†é¡", 
            "game_code": "N/A", 
            "promoter": "N/A",
            "venue_info": "",
            "location": ""
        }
        
    def extract_event_info(self, lines, js_data=None):
        """ã€åˆä½µç‰ˆ+æš´åŠ›è£œå…¨ã€‘æ¼”å‡ºè³‡è¨Šæå– - çµ±ä¸€è™•ç†æ—¥æœŸæ™‚é–“è³‡è¨Š"""
        event_info_lines = []
        
        # ç¬¬ä¸€å„ªå…ˆæ¬Šï¼šæ­£å¸¸çš„æ—¥æœŸæ™‚é–“åŒ¹é…
        datetime_patterns = [
            r'\d{4}/\d{1,2}/\d{1,2}',     # YYYY/MM/DD æ—¥æœŸæ ¼å¼
            r'\d{1,2}/\d{1,2}/\d{4}',     # MM/DD/YYYY æ—¥æœŸæ ¼å¼  
            r'\d{1,2}/\d{1,2}',          # MM/DD ç°¡åŒ–æ—¥æœŸæ ¼å¼
            r'\d{4}-\d{1,2}-\d{1,2}',     # YYYY-MM-DD æ—¥æœŸæ ¼å¼
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥', # YYYYå¹´MMæœˆDDæ—¥ ä¸­æ–‡æ—¥æœŸæ ¼å¼
            r'\d{1,2}æœˆ\d{1,2}æ—¥',        # MMæœˆDDæ—¥ ä¸­æ–‡ç°¡åŒ–æ ¼å¼
            r'\d{1,2}:\d{2}',           # HH:MM æ™‚é–“æ ¼å¼
            r'\d{1,2}ï¼š\d{2}'           # HHï¼šMM ä¸­æ–‡å†’è™Ÿæ™‚é–“æ ¼å¼
        ]
        
        # ç›¸é—œé—œéµå­—ï¼ˆæ¼”å‡ºè³‡è¨Šç›¸é—œï¼‰
        event_keywords = [
            'æ¼”å‡ºæ—¥æœŸ', 'æ´»å‹•æ—¥æœŸ', 'èˆ‰è¾¦æ—¥æœŸ', 'è¡¨æ¼”æ—¥æœŸ', 'é–‹æ¼”æ—¥æœŸ',
            'æ¼”å‡ºæ™‚é–“', 'é–‹æ¼”æ™‚é–“', 'è¡¨æ¼”æ™‚é–“', 'æ´»å‹•æ™‚é–“',
            'æ—¥æœŸ', 'æ™‚é–“', 'é–‹å§‹æ™‚é–“', 'æ´»å‹•è³‡è¨Š'
        ]
        
        for line in lines:
            # æ’é™¤å¹²æ“¾å…§å®¹
            if any(exclude in line for exclude in ['é€€ç¥¨', 'æ‰‹çºŒè²»', 'ibon', 'å®¢æœ', 'æ³¨æ„äº‹é …']):
                continue
                
            # æª¢æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸæ™‚é–“æ ¼å¼æˆ–ç›¸é—œé—œéµå­—
            has_datetime = any(re.search(pattern, line) for pattern in datetime_patterns)
            has_keyword = any(keyword in line for keyword in event_keywords)
            
            if has_datetime or has_keyword:
                clean_line = self.clean_text(line)
                if clean_line and clean_line not in event_info_lines:
                    event_info_lines.append(clean_line)
        
        # ã€æš´åŠ›ä¿åº•ã€‘å¦‚æœJSæŠ“ä¸åˆ°æˆ–å…§å®¹ä¸è¶³ï¼Œä½¿ç”¨introå‰5è¡Œä½œç‚ºä¿åº•
        if (not event_info_lines or 
            (js_data and js_data.get('title') in ['JSæå–å¤±æ•—', 'JSç›£æ§è¶…æ™‚', 'æœªæŠ“åˆ°æ¨™é¡Œ'])):
            
            self.logger.info("  [æš´åŠ›ä¿åº•] ä½¿ç”¨introå‰5è¡Œä½œç‚ºevent_infoä¿åº•")
            fallback_lines = []
            for i, line in enumerate(lines[:5]):  # å–å‰5è¡Œ
                if line.strip() and len(line.strip()) > 5:  # éæ¿¾æ‰å¤ªçŸ­çš„è¡Œ
                    clean_line = self.clean_text(line)
                    if clean_line not in fallback_lines:
                        fallback_lines.append(clean_line)
            
            if fallback_lines:
                if event_info_lines:
                    # å¦‚æœå·²æœ‰éƒ¨åˆ†è³‡è¨Šï¼Œå°‡ä¿åº•è³‡è¨Šè¿½åŠ 
                    event_info_lines.extend(fallback_lines)
                else:
                    # å¦‚æœå®Œå…¨æ²’æœ‰è³‡è¨Šï¼Œä½¿ç”¨ä¿åº•è³‡è¨Š
                    event_info_lines = fallback_lines
        
        # å»é‡è¤‡æ¸…æ´—ï¼šç§»é™¤é‡è¤‡çš„å‰ç¶´è©
        cleaned_lines = []
        for line in event_info_lines:
            # ç§»é™¤é‡è¤‡çš„å‰ç¶´è©
            prefixes_to_remove = ['æ¼”å‡ºè³‡è¨Šï¼š', 'æ™‚é–“ï¼š', 'æ—¥æœŸï¼š', 'æ´»å‹•è³‡è¨Šï¼š', 'EVENT INFOï¼š']
            clean_line = line
            for prefix in prefixes_to_remove:
                if clean_line.startswith(prefix):
                    clean_line = clean_line[len(prefix):].strip()
                    break
            
            if clean_line and clean_line not in cleaned_lines:
                cleaned_lines.append(clean_line)
        
        # ã€çµæ§‹å”¯ä¸€åŒ–ã€‘éæ¿¾ç©ºæ®¼å’Œé‡è¤‡æª¢æŸ¥
        cleaned_lines = self.filter_empty_shells(cleaned_lines)
        cleaned_lines = self.remove_duplicate_lines(cleaned_lines)
        
        # åˆä½µæ‰€æœ‰ç›¸é—œè³‡è¨Šï¼Œç”¨åˆ†è™Ÿåˆ†éš”
        if cleaned_lines:
            return ' ; '.join(cleaned_lines[:8])  # æœ€å¤šä¿ç•™å‰8è¡Œï¼Œé¿å…éåº¦å†—é•·
        
        return "æœªæ‰¾åˆ°"
        
    def extract_precise_price(self, lines):
        """ã€æš´åŠ›ç‰ˆ+å”¯ä¸€åŒ–ã€‘æ´»å‹•ç¥¨åƒ¹æå– - å¯§å¯æŠ“éŒ¯çµ•ä¸æ¼æ‰"""
        price_lines = []
        
        # ç¬¬ä¸€å„ªå…ˆæ¬Šï¼šåŒ…å«è²¨å¹£ç¬¦è™Ÿçš„è¡Œ
        for line in lines:
            price_matches = re.findall(r'NT\$\s?[\d,]+|[\d,]+å…ƒ', line)
            if price_matches:
                price_keywords = ['ç¥¨åƒ¹', 'Price', 'NT$', 'å…ƒ', 'VIP', 'VVIP', 'Aå€', 'Bå€']
                if any(keyword in line for keyword in price_keywords):
                    clean_line = self.clean_text(line)
                    if clean_line not in price_lines:
                        price_lines.append(clean_line)
        
        # ç¬¬äºŒå„ªå…ˆæ¬Šï¼šæš´åŠ›æœç´¢ç¥¨åƒ¹ç›¸é—œé—œéµå­—ï¼ˆå³ä½¿æ²’æœ‰NT$ï¼‰
        if not price_lines:
            price_keywords_extended = [
                'ç¥¨åƒ¹', 'PRICE', 'Price', 'å…è²»', 'FREE', 'Free',
                'è²»ç”¨', 'åƒ¹æ ¼', 'æ”¶è²»', 'é‡‘é¡', 'å®šåƒ¹', 'å”®åƒ¹',
                'VIP', 'VVIP', 'GA', 'é å”®', 'ç¾å ´ç¥¨', 'èº«éšœ'
            ]
            
            for line in lines:
                # æ’é™¤æ˜é¡¯çš„å¹²æ“¾å…§å®¹
                if any(exclude in line for exclude in ['é€€ç¥¨', 'æ‰‹çºŒè²»', 'æ³¨æ„äº‹é …', 'è¦å®š']):
                    continue
                    
                # åªè¦åŒ…å«åƒ¹æ ¼é—œéµå­—å°±æŠ“
                if any(keyword in line for keyword in price_keywords_extended):
                    clean_line = self.clean_text(line)
                    if clean_line and clean_line not in price_lines:
                        price_lines.append(clean_line)
        
        # ã€çµæ§‹å”¯ä¸€åŒ–ã€‘éæ¿¾ç©ºæ®¼å’Œé‡è¤‡æª¢æŸ¥
        price_lines = self.filter_empty_shells(price_lines)
        price_lines = self.remove_duplicate_lines(price_lines)
        price_lines = [line for line in price_lines if self.is_valid_price_line(line)]
        
        # åˆä½µæ‰€æœ‰æ‰¾åˆ°çš„åƒ¹æ ¼è³‡è¨Š
        if price_lines:
            return ' ; '.join(price_lines)
        
        return "æœªæ‰¾åˆ°"

    def is_valid_price_line(self, line):
        """åƒ¹æ ¼æ¬„ä½æ¸…æ´—ï¼šæ’é™¤é å”®è³‡æ ¼èˆ‡è¨»å†Šèªªæ˜ç­‰éç¥¨åƒ¹å…§å®¹"""
        if not line:
            return False

        line = self.clean_text(line)
        if not line:
            return False

        noise_keywords = [
            'é å”®è³‡æ ¼', 'é å”®è³¼ç¥¨', 'è³¼ç¥¨åºè™Ÿ', 'é›»å­éƒµä»¶', 'ç™»è¨˜', 'æœƒå“¡é è³¼',
            'presale access', 'register now', 'registered users', 'registration',
            'è«‹å…ˆå®Œæˆç™»è¨˜', 'å…Œæ›è³‡æ ¼åˆ¸', 'æ•¸é‡æœ‰é™ï¼Œæ›å®Œç‚ºæ­¢'
        ]

        if any(keyword.lower() in line.lower() for keyword in noise_keywords):
            return False

        has_currency = bool(re.search(r'nt\$\s?[\d,]+|\$\s?[\d,]+|[\d,]+\s*å…ƒ', line.lower()))
        has_price_token = any(token in line.lower() for token in [
            'ç¥¨åƒ¹', 'price', 'vip', 'vvip', 'ga', 'cat', 'èº«éšœ', 'æ—©é³¥', 'å…¨ç¥¨', 'å­¸ç”Ÿç¥¨'
        ])

        if has_currency:
            return True

        return has_price_token and len(line) <= 120
        
    def extract_precise_location(self, lines, js_data=None, url=None, title=None):
        """ã€æš´åŠ›ç‰ˆã€‘æ¼”å‡ºåœ°é»æå– - æ“´å¤§æœç´¢ç¯„åœ"""
        location_lines = []
        
        # ç¬¬ä¸€å„ªå…ˆæ¬Šï¼šJS dataLayer è³‡è¨Š
        if js_data:
            if js_data.get('venue_info') and js_data['venue_info'].strip():
                venue_info = self.clean_text(js_data['venue_info'])
                simplified = self.simplify_location(venue_info)
                if simplified != "æœªæ‰¾åˆ°" and self.is_valid_location_line(simplified):
                    return simplified
            if js_data.get('location') and js_data['location'].strip():
                location = self.clean_text(js_data['location'])
                simplified = self.simplify_location(location)
                if simplified != "æœªæ‰¾åˆ°" and self.is_valid_location_line(simplified):
                    return simplified
        
        # ç¬¬äºŒå„ªå…ˆæ¬Šï¼šæ¨™æº–å ´é¤¨é—œéµå­—
        location_keywords_primary = [
            'åœ°é»', 'Venue', 'é¤¨', 'é«”è‚²å ´', 'ä¸­å¿ƒ', 'Legacy', 'Zepp', 
            'æ¼”å‡ºåœ°é»', 'æ´»å‹•åœ°é»', 'èˆ‰è¾¦åœ°é»', 'å ´åœ°', 'Arena', 'å·¨è›‹',
            'éŸ³æ¨‚ä¸­å¿ƒ', 'å±•è¦½é¤¨', 'é«”è‚²é¤¨', 'æ¼”è—å»³', 'åŠ‡é™¢', 'éŸ³æ¨‚å»³',
            'å·¥å•†å±•è¦½', 'TICC', 'ATT', 'æµè¡ŒéŸ³æ¨‚', 'å°å·¨è›‹'
        ]
        
        for line in lines:
            if any(keyword in line for keyword in location_keywords_primary):
                if 'åœ°é»' in line and 'æ³¨æ„äº‹é …' in line:
                    location_part = line.split('æ³¨æ„äº‹é …')[0]
                    clean_location = self.clean_text(location_part)
                    simplified = self.simplify_location(clean_location)
                    if simplified not in location_lines and self.is_valid_location_line(simplified):
                        location_lines.append(simplified)
                else:
                    clean_location = self.clean_text(line)
                    simplified = self.simplify_location(clean_location)
                    if simplified not in location_lines and self.is_valid_location_line(simplified):
                        location_lines.append(simplified)
        
        # ç¬¬ä¸‰å„ªå…ˆæ¬Šï¼šæš´åŠ›æœç´¢æ“´å±•é—œéµå­—
        if not location_lines:
            location_keywords_extended = [
                'åœ°å€', 'VENUE', 'ADD', 'Address', 'æ»‘é›ªå ´', 'å±•è¦½ä¸­å¿ƒ',
                'æœƒè­°ä¸­å¿ƒ', 'æ´»å‹•ä¸­å¿ƒ', 'åœ‹éš›æœƒè­°', 'æµè¡ŒéŸ³æ¨‚', 'æ–‡åŒ–ä¸­å¿ƒ',
                'SUB LIVE', 'Backstage', 'å¾Œå°', 'å·¨è›‹', 'Dome', 
                'æ–°åŒ—å¸‚', 'å°åŒ—å¸‚', 'é«˜é›„å¸‚', 'å°ä¸­å¸‚', 'æ–°ç«¹', 'å°å—',
                'å ´é¤¨', 'Stadium', 'Hall', 'å»³', 'å®®'
            ]
            
            for line in lines:
                # æ’é™¤å¹²æ“¾å…§å®¹
                if any(exclude in line for exclude in ['é€€ç¥¨', 'æ‰‹çºŒè²»', 'æ³¨æ„äº‹é …', 'è¦å®š', 'æé†’']):
                    continue
                    
                if any(keyword in line for keyword in location_keywords_extended):
                    clean_location = self.clean_text(line)
                    simplified = self.simplify_location(clean_location)
                    if simplified and simplified not in location_lines and self.is_valid_location_line(simplified):
                        location_lines.append(simplified)
        
        # JS å‚™æ´ï¼šä¸»è¾¦æ–¹åœ°ç†ç·šç´¢
        if not location_lines and js_data and js_data.get('promoter') and js_data['promoter'] != 'N/A':
            promoter = js_data['promoter']
            if any(venue in promoter for venue in ['å°åŒ—', 'é«˜é›„', 'å°ä¸­', 'æ–°åŒ—', 'æ–°ç«¹', 'å°å—']):
                location_lines.append(self.clean_text(f"ä¸»è¾¦æ–¹ç·šç´¢ï¼š{promoter}"))

        # URL / æ¨™é¡Œå›é€€ï¼šè™•ç†é é¢æœªæ¨™è¨»å ´åœ°ä½†æ´»å‹•æœ¬èº«å¯åˆ¤æ–·çš„æƒ…æ³
        if not location_lines:
            inferred_location = self.infer_location_from_context(url=url, title=title)
            if inferred_location and self.is_valid_location_line(inferred_location):
                location_lines.append(inferred_location)
        
        # åˆä½µçµæœ - ã€çµæ§‹å”¯ä¸€åŒ–ã€‘
        if location_lines:
            # éæ¿¾ç©ºæ®¼å’Œå»é‡è¤‡
            location_lines = self.filter_empty_shells(location_lines)
            location_lines = self.remove_duplicate_lines(location_lines)
            location_lines = [line for line in location_lines if self.is_valid_location_line(line)]
            return ' ; '.join(location_lines[:2])  # æœ€å¤šé¡¯ç¤ºå‰2å€‹åœ°é»è³‡è¨Š
        
        return "æœªæ‰¾åˆ°"

    def infer_location_from_context(self, url=None, title=None):
        """ä»¥ URL / æ¨™é¡Œåšåœ°é»ä¿åº•æ¨æ–·ï¼Œé¿å…é—œéµè³‡è¨Šç¼ºæ¼ã€‚"""
        normalized_url = (url or "").lower()
        normalized_title = self.clean_text(title or "").lower()

        # å¤§æ¸¯é–‹å”±ç³»åˆ—é é¢å¸¸ç¼ºå°‘æ˜ç¢ºã€Œåœ°é»ã€æ¬„ä½
        if any(keyword in normalized_url for keyword in ["/26_megaport", "/26_megaport_c", "/26_megaport_d"]):
            return "é«˜é›„é§äºŒè—è¡“ç‰¹å€"

        if "å¤§æ¸¯é–‹å”±" in normalized_title or "megaport" in normalized_title:
            return "é«˜é›„é§äºŒè—è¡“ç‰¹å€"

        return None
    
    def simplify_location(self, location_text):
        """ã€ç²¾æº–åœ°é»ç‰ˆã€‘åœ°é»è³‡è¨Šç²¾ç…‰ - æå–æ ¸å¿ƒå ´é¤¨è³‡è¨Š"""
        if not location_text or location_text == "æœªæ‰¾åˆ°":
            return location_text
        
        # ç²¾æº–åœ°é»æå–ï¼šå¦‚æœå­—æ•¸è¶…é20å­—ä¸”åŒ…å«å½¢å®¹è©ï¼Œåƒ…æå–å ´é¤¨éƒ¨åˆ†
        if len(location_text) > 20:
            # å®£å‚³å½¢å®¹è©é—œéµå­—
            promotional_keywords = ['ç½é•', 'éœ‡æ’¼', 'å£ç¢‘', 'æ»¿è¼‰', 'ç†±åŠ›', 'ç²¾å½©', 'çµ•ä½³', 'å®Œç¾', 'ç¶“å…¸', 'å‚³å¥‡', 'å¤¢å¹»', 'é ‚ç´š', 'è±ªè¯']
            
            # å¦‚æœåŒ…å«å®£å‚³è©ï¼Œå˜—è©¦æå–æ ¸å¿ƒå ´é¤¨è³‡è¨Š
            if any(keyword in location_text for keyword in promotional_keywords):
                # ä½¿ç”¨ Regex æå–åŒ…å«å ´é¤¨é—œéµå­—çš„éƒ¨åˆ†
                venue_patterns = [
                    r'[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ]*(?:é¤¨|å·¨è›‹|ä¸­å¿ƒ|å±•è¦½é¤¨|Arena|Hall|Stadium|Dome|TICC|ATT|Legacy|Zepp)[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ]*',
                    r'[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ]*(?:å°åŒ—|é«˜é›„|å°ä¸­|æ–°åŒ—|æ–°ç«¹|å°å—)[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ]*(?:é¤¨|å·¨è›‹|ä¸­å¿ƒ|å±•è¦½é¤¨|Arena|Hall)[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ]*',
                    r'[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ]*(?:å°å·¨è›‹|å¤§å·¨è›‹|é«”è‚²å ´|éŸ³æ¨‚ä¸­å¿ƒ|å±•è¦½ä¸­å¿ƒ|æœƒè­°ä¸­å¿ƒ)[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ]*'
                ]
                
                for pattern in venue_patterns:
                    match = re.search(pattern, location_text)
                    if match:
                        extracted = match.group().strip()
                        if extracted and len(extracted) > 3:
                            self.logger.debug(f"åœ°é»ç²¾ç…‰ï¼š{location_text[:30]}... â†’ {extracted}")
                            return extracted
        
        # æ¨™æº–æˆªæ–·è™•ç†
        if len(location_text) > 50:
            # é€€ç¥¨è¦ç¯„é—œéµå­—
            refund_keywords = ['é€€ç¥¨', 'é€€æ›', 'æ‰‹çºŒè²»', 'è¦å®š', 'æ”¿ç­–', 'æ¢æ¬¾', 'èªªæ˜', 'æ³¨æ„äº‹é …', 'æé†’', 'é ˆçŸ¥']
            
            # å¦‚æœåŒ…å«é€€ç¥¨è¦ç¯„ï¼Œåœ¨ç¬¬ä¸€å€‹é€€ç¥¨é—œéµå­—å‰æˆªæ–·
            for keyword in refund_keywords:
                if keyword in location_text:
                    truncated = location_text.split(keyword)[0].strip()
                    if truncated:
                        self.logger.debug(f"åœ°é»è³‡è¨Šå·²æˆªæ–·ï¼š{len(location_text)} å­— â†’ {len(truncated)} å­—")
                        return truncated
                    break
            
            # å¦‚æœæ²’æœ‰é€€ç¥¨é—œéµå­—ä½†è¶…é 50 å­—ï¼Œç›´æ¥æˆªå–å‰ 50 å­—
            if len(location_text) > 50:
                truncated = location_text[:50] + "..."
                self.logger.debug(f"åœ°é»è³‡è¨Šå·²æˆªæ–·è‡³ 50 å­—ï¼š{truncated}")
                return truncated
        
        return location_text

    def is_valid_location_line(self, line):
        """åœ°é»æ¬„ä½æ¸…æ´—ï¼šä¿ç•™å ´é¤¨/åœ°æ¨™è³‡è¨Šï¼Œæ’é™¤å”®ç¥¨èˆ‡è¦ç¯„é›œè¨Š"""
        if not line:
            return False

        line = self.clean_text(line)
        if not line or line == "æœªæ‰¾åˆ°":
            return False

        lowered = line.lower()
        noise_keywords = [
            'è³¼ç¥¨åºè™Ÿ', 'é å”®è³‡æ ¼', 'é å”®è³¼ç¥¨', 'é›»å­éƒµä»¶', 'æœƒå“¡é è³¼', 'ç«‹å³ç™»è¨˜',
            'é€€ç¥¨', 'æ‰‹çºŒè²»', 'æ³¨æ„äº‹é …', 'å®‰æª¢', 'é©—åŒ…', 'ç¦æ­¢æ”œå¸¶', 'ç›´æ’­',
            'presale access', 'register now', 'registered users', 'via email', 'security check'
        ]
        if any(keyword.lower() in lowered for keyword in noise_keywords):
            return False

        venue_tokens = [
            'åœ°é»', 'venue', 'å ´åœ°', 'å ´é¤¨', 'é¤¨', 'ä¸­å¿ƒ', 'é«”è‚²å ´', 'é«”è‚²é¤¨',
            'arena', 'hall', 'dome', 'legacy', 'zepp', 'sub live', 'å°å·¨è›‹', 'å¤§å·¨è›‹',
            'é§äºŒ', 'å±•è¦½é¤¨', 'å±•è¦½ä¸­å¿ƒ', 'éŸ³æ¨‚ä¸­å¿ƒ', 'æ¼”è—å»³', 'åŠ‡é™¢', 'ticc', 'att'
        ]
        has_venue_token = any(token.lower() in lowered for token in venue_tokens)

        # éé•·ä¸”æ²’æœ‰æ˜ç¢ºå ´é¤¨è©å½™ï¼Œå¤šç‚ºæ•˜äº‹æˆ–è¦ç¯„å…§å®¹
        if len(line) > 90 and not has_venue_token:
            return False

        return has_venue_token or len(line) <= 30
        
    def extract_precise_sale_time(self, lines, js_data=None):
        """ã€å¤šè¡Œç´¯ç©ç‰ˆ+å”¯ä¸€åŒ–ã€‘å”®ç¥¨æ™‚é–“æå– - å¼·åŠ›é»è‘—é‚è¼¯"""
        sale_time_lines = []
        
        # JS å‚™æ´åŠ å¼·ï¼šæª¢æŸ¥ dataLayer çš„å”®ç¥¨ç›¸é—œæ¬„ä½
        if js_data:
            js_sale_fields = ['onSaleTime', 'saleTime', 'ticketSaleTime', 'presaleTime']
            for field in js_sale_fields:
                if js_data.get(field) and js_data[field].strip():
                    sale_info = self.clean_text(js_data[field])
                    if sale_info not in sale_time_lines and self.is_valid_sale_time_line(sale_info):
                        sale_time_lines.append(sale_info)
        
        # å”®ç¥¨é—œéµå­—ï¼ˆä¸­è‹±æ–‡æ··åˆï¼Œå¤šæ¨£åŒ–ï¼‰
        sale_keywords = [
            'å”®ç¥¨', 'é–‹è³£', 'å•Ÿå”®', 'é–‹å”®', 'è³¼ç¥¨', 'é å”®', 'æœƒå“¡è³¼',
            'PRESALE', 'ON SALE', 'SALE', 'Presale', 'On Sale',
            'æ¶ç¥¨', 'è²·ç¥¨', 'è¨‚ç¥¨', 'è²©å”®', 'éŠ·å”®', 'ç™¼å”®',
            'æœƒå“¡é è³¼', 'å„ªå…ˆè³¼', 'å…¨é¢é–‹è³£', 'æ­£å¼é–‹è³£', 'é™æ™‚é è³¼'
        ]
        
        # å¤šè¡Œç´¯ç©ï¼šæœé›†æ‰€æœ‰å”®ç¥¨ç›¸é—œè³‡è¨Š
        for line in lines:
            # æ’é™¤æ˜é¡¯å¹²æ“¾å…§å®¹
            if any(exclude in line for exclude in ['é€€ç¥¨', 'æ‰‹çºŒè²»', 'æ³¨æ„äº‹é …', 'è¦å®š', 'æé†’']):
                continue
                
            # æª¢æŸ¥å”®ç¥¨é—œéµå­—
            if any(keyword in line for keyword in sale_keywords):
                clean_line = self.clean_text(line)
                if clean_line and clean_line not in sale_time_lines:
                    sale_time_lines.append(clean_line)
        
        # æ—¥æœŸæ ¼å¼æ”¾å¯¬æª¢æ¸¬ï¼šå°‹æ‰¾åŒ…å«æ—¥æœŸçš„å”®ç¥¨è³‡è¨Š
        if not sale_time_lines:
            date_patterns_extended = [
                r'\d{4}\.\d{1,2}\.\d{1,2}',    # YYYY.MM.DD
                r'\d{1,2}/\d{1,2}',            # MM/DD
                r'\d{1,2}æœˆ\d{1,2}æ—¥',          # MæœˆDæ—¥
                r'\d{4}/\d{1,2}/\d{1,2}',      # YYYY/MM/DD
                r'\d{1,2}:\d{2}',             # HH:MM
                r'\d{4}-\d{1,2}-\d{1,2}'       # YYYY-MM-DD
            ]
            
            for line in lines:
                # å¦‚æœåŒ…å«æ—¥æœŸæ ¼å¼ï¼Œå¯èƒ½æ˜¯å”®ç¥¨è³‡è¨Š
                if any(re.search(pattern, line) for pattern in date_patterns_extended):
                    # é€²ä¸€æ­¥æª¢æŸ¥æ˜¯å¦å¯èƒ½èˆ‡å”®ç¥¨ç›¸é—œ
                    potential_keywords = ['æ™‚é–“', 'æ—¥æœŸ', 'é–‹å§‹', 'æˆªæ­¢', 'æœŸé–“', 'éšæ®µ']
                    if any(keyword in line for keyword in potential_keywords):
                        clean_line = self.clean_text(line)
                        if clean_line and clean_line not in sale_time_lines:
                            sale_time_lines.append(clean_line)
        
        # ã€çµæ§‹å”¯ä¸€åŒ–ã€‘éæ¿¾ç©ºæ®¼å’Œé‡è¤‡æª¢æŸ¥
        sale_time_lines = self.filter_empty_shells(sale_time_lines)
        sale_time_lines = self.remove_duplicate_lines(sale_time_lines)
        sale_time_lines = [line for line in sale_time_lines if self.is_valid_sale_time_line(line)]
        
        # å»é‡è¤‡æ¸…æ´—ï¼šç§»é™¤é‡è¤‡çš„å‰ç¶´è©
        cleaned_lines = []
        for line in sale_time_lines:
            # ç§»é™¤é‡è¤‡çš„å‰ç¶´è©
            prefixes_to_remove = ['å”®ç¥¨æ™‚é–“ï¼š', 'æ™‚é–“ï¼š', 'é–‹è³£æ™‚é–“ï¼š', 'å”®ç¥¨ï¼š', 'SALE TIMEï¼š']
            clean_line = line
            for prefix in prefixes_to_remove:
                if clean_line.startswith(prefix):
                    clean_line = clean_line[len(prefix):].strip()
                    break
            
            if clean_line and clean_line not in cleaned_lines:
                cleaned_lines.append(clean_line)
        
        # åˆä½µçµæœ
        if cleaned_lines:
            return ' ; '.join(cleaned_lines)
        
        return "æœªæ‰¾åˆ°"

    def is_valid_sale_time_line(self, line):
        """å”®ç¥¨æ™‚é–“æ¬„ä½æ¸…æ´—ï¼šä¿ç•™å«æ—¥æœŸ/æ™‚é–“çš„å”®ç¥¨ç¯€é»ï¼Œæ’é™¤è¨»å†Šèˆ‡ä¿ƒéŠ·èªªæ˜"""
        if not line:
            return False

        line = self.clean_text(line)
        if not line or line == "æœªæ‰¾åˆ°":
            return False

        lowered = line.lower()
        noise_keywords = [
            'é å”®è³‡æ ¼é™é‡ç™¼é€ä¸­', 'ç«‹å³ç™»è¨˜', 'ç™»è¨˜æˆåŠŸ', 'è³¼ç¥¨åºè™Ÿ', 'é›»å­éƒµä»¶',
            'æ•¸é‡çš†æœ‰é™ï¼Œå”®å®Œç‚ºæ­¢', 'ä¸ä¿è­‰åº§ä½æ’è™Ÿ', 'vip upgrade', 'presale access registration',
            'register now', 'registered users', 'via email'
        ]
        if any(keyword.lower() in lowered for keyword in noise_keywords):
            return False

        has_datetime = bool(re.search(
            r'\d{4}[./-]\d{1,2}[./-]\d{1,2}|\d{1,2}[./-]\d{1,2}|\d{1,2}:\d{2}|\d{1,2}æœˆ\d{1,2}æ—¥',
            line
        ))
        sale_tokens = [
            'å”®ç¥¨', 'é–‹è³£', 'é å”®', 'æœƒå“¡é å”®', 'æ­£å¼é–‹è³£', 'å…¨é¢é–‹è³£',
            'presale', 'on sale', 'sale time'
        ]
        has_sale_token = any(token.lower() in lowered for token in sale_tokens)

        # å”®ç¥¨æ™‚é–“æ¬„ä½ä»¥æ™‚é–“ç¯€é»ç‚ºä¸»ï¼Œç´”èªªæ˜å¥ä¸ä¿ç•™
        if has_datetime and has_sale_token:
            return True

        return has_sale_token and len(line) <= 24
        
    def extract_precise_time_backup(self, lines):
        """æ¼”å‡ºæ™‚é–“å‚™ç”¨æå– - å·²æ•´åˆè‡³ event_info"""
        show_keywords = ['æ¼”å‡º', 'é–‹æ¼”', 'è¡¨æ¼”']  
        time_pattern = r'\d{1,2}:\d{2}'
        
        for line in lines:
            if any(keyword in line for keyword in show_keywords) and re.search(time_pattern, line):
                return self.clean_text(line)
                    
        return "æœªæ‰¾åˆ°"
        
        
    def extract_all_text_from_intro(self, url):
        """å¾introå€å¡Šæå–æ‰€æœ‰æ–‡å­—è¡Œ"""
        try:
            self.driver.get(url)
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # æå–æ¨™é¡Œ
            title = "æœªæ‰¾åˆ°"
            try:
                title_element = self.driver.find_element(By.ID, "synopsisEventTitle")
                title = self.clean_text(title_element.text)
            except NoSuchElementException:
                try:
                    title_element = self.driver.find_element(By.TAG_NAME, "h1")
                    title = self.clean_text(title_element.text)
                except NoSuchElementException:
                    pass
            
            # æå–introå€å¡Šæ–‡å­—
            lines = []
            try:
                intro_element = self.driver.find_element(By.ID, "intro")
                all_text = intro_element.text
                
                # æŒ‰æ›è¡Œåˆ†å‰²æˆç¨ç«‹è¡Œ
                lines = [line.strip() for line in all_text.split('\n') if line.strip()]
                
            except NoSuchElementException:
                self.logger.warning(f"æœªæ‰¾åˆ°introå€å¡Šï¼š{url}")
                
            return title, lines
            
        except Exception as e:
            self.logger.error(f"æå–æ–‡å­—å¤±æ•— {url}: {e}")
            return "éŒ¯èª¤", []
            
    def process_single_event(self, url, index):
        """è™•ç†å–®ä¸€æ´»å‹•çš„å®Œæ•´é‚è¼¯ - å¼·åŒ–ç‰ˆ"""
        try:
            self.logger.info(f"è™•ç†æ´»å‹• {index}ï¼š{url}")
            
            # é€²å…¥é é¢ä¸¦ç­‰å¾…JavaScriptè¼‰å…¥
            self.driver.get(url)
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(random.uniform(3, 6))  # éš¨æ©Ÿç­‰å¾… 3-6 ç§’
            
            # å„ªå…ˆä½¿ç”¨ JavaScript dataLayer æå–ç²¾ç¢ºæ•¸æ“š
            js_data = self.get_clean_data_from_js(self.driver)
            
            # æå–HTMLåŸºæœ¬è³‡è¨Šä½œç‚ºå‚™ç”¨
            title, lines = self.extract_all_text_from_intro(url)
            
            # æ¨™é¡Œå¼·åˆ¶è£œå…¨ï¼šçµ•å°ä¸å…è¨±ã€æœªæ‰¾åˆ°ã€
            final_title = "æœªæ‰¾åˆ°"  # é è¨­å€¼
            
            # ç¬¬ä¸€å„ªå…ˆæ¬Šï¼šJSæ¨™é¡Œ
            if (js_data.get('title') and 
                js_data['title'] not in ['æœªæŠ“åˆ°æ¨™é¡Œ', 'JSæå–å¤±æ•—', 'JSç›£æ§è¶…æ™‚', ''] and 
                js_data['title'].strip()):
                final_title = js_data['title']
                self.logger.info(f"  [JSå„ªå…ˆ] ä½¿ç”¨æ¨™é¡Œ: {final_title}")
            # ç¬¬äºŒå„ªå…ˆæ¬Šï¼šHTMLæ¨™é¡Œ
            elif title and title not in ['æœªæ‰¾åˆ°', 'éŒ¯èª¤', ''] and title.strip():
                final_title = title
                self.logger.info(f"  [HTMLå‚™ç”¨] ä½¿ç”¨æ¨™é¡Œ: {final_title}")
            else:
                # å‚™æ´æ–¹æ¡ˆï¼šå¼·åˆ¶ä½¿ç”¨ç€è¦½å™¨åˆ†é æ¨™é¡Œ
                try:
                    browser_title = self.driver.title
                    if browser_title and browser_title.strip():
                        # æ¸…ç†ç€è¦½å™¨æ¨™é¡Œ
                        clean_browser_title = browser_title.replace('æ‹“å…ƒå”®ç¥¨ç¶²', '').replace('TIXCRAFT', '').strip()
                        if clean_browser_title:
                            final_title = clean_browser_title
                            self.logger.info(f"  [ç€è¦½å™¨å‚™æ´] ä½¿ç”¨æ¨™é¡Œ: {final_title}")
                        else:
                            # æœ€å¾Œå‚™æ´ï¼šå¾URLæå–æ´»å‹•ä»£ç¢¼ä½œç‚ºæ¨™é¡Œ
                            url_code = url.split('/')[-1] if '/' in url else 'unknown'
                            final_title = f"æ´»å‹•ä»£ç¢¼ï¼š{url_code}"
                            self.logger.info(f"  [URLæœ€çµ‚å‚™æ´] ä½¿ç”¨æ¨™é¡Œ: {final_title}")
                    else:
                        # æœ€å¾Œå‚™æ´ï¼šå¾URLæå–æ´»å‹•ä»£ç¢¼ä½œç‚ºæ¨™é¡Œ
                        url_code = url.split('/')[-1] if '/' in url else 'unknown'
                        final_title = f"æ´»å‹•ä»£ç¢¼ï¼š{url_code}"
                        self.logger.info(f"  [URLæœ€çµ‚å‚™æ´] ä½¿ç”¨æ¨™é¡Œ: {final_title}")
                except Exception as e:
                    # å¦‚æœé€£ç€è¦½å™¨æ¨™é¡Œéƒ½æŠ“ä¸åˆ°ï¼Œä½¿ç”¨URLä»£ç¢¼
                    url_code = url.split('/')[-1] if '/' in url else 'unknown'
                    final_title = f"æ´»å‹•ä»£ç¢¼ï¼š{url_code}"
                    self.logger.warning(f"  [ç•°å¸¸å‚™æ´] ç€è¦½å™¨æ¨™é¡Œæå–å¤±æ•—({e})ï¼Œä½¿ç”¨: {final_title}")
            
            if not lines:
                return {
                    'index': index,
                    'title': final_title,
                    'event_info': "æœªæ‰¾åˆ°",
                    'location': "æœªæ‰¾åˆ°",
                    'price': "æœªæ‰¾åˆ°",
                    'sale_time': "æœªæ‰¾åˆ°",
                    'url': url
                }
            
            # ä½¿ç”¨ç²¾ç¢ºæå–è¦å‰‡ï¼Œé˜²æ­¢ä½ç§»å•é¡Œ - ã€åˆä½µç‰ˆ+æš´åŠ›è£œå…¨ã€‘
            event_info = self.extract_event_info(lines, js_data)  # åˆä½µæ—¥æœŸæ™‚é–“è³‡è¨Š + JSæ•¸æ“š
            
            # ä¿®æ­£åœ°é»æå–ï¼Œå‚³å…¥ js_data
            location = self.extract_precise_location(lines, js_data, url=url, title=final_title)
            price = self.extract_precise_price(lines)
            sale_time = self.extract_precise_sale_time(lines, js_data)  # å‚³å…¥ js_data æ”¯æ´
            
            result = {
                'index': index,
                'title': final_title,
                'event_info': event_info,  # åˆä½µçš„æ¼”å‡ºè³‡è¨Š
                'location': location,
                'price': price,
                'sale_time': sale_time,
                'url': url
            }
            
            # æ—¥èªŒè¼¸å‡º - æ›´æ–°ç‚ºæ–°æ ¼å¼
            self.logger.info(f"  æ¨™é¡Œï¼š{final_title}")
            self.logger.info(f"  JSåˆ†é¡ï¼š{js_data.get('category', 'N/A')}")
            self.logger.info(f"  éŠæˆ²ä»£ç¢¼ï¼š{js_data.get('game_code', 'N/A')}")
            self.logger.info(f"  ä¸»è¾¦æ–¹ï¼š{js_data.get('promoter', 'N/A')}")
            self.logger.info(f"  æ¼”å‡ºè³‡è¨Šï¼š{event_info}")  # åˆä½µé¡¯ç¤º
            self.logger.info(f"  åœ°é»ï¼š{location}")
            self.logger.info(f"  ç¥¨åƒ¹ï¼š{price}")
            self.logger.info(f"  å”®ç¥¨æ™‚é–“ï¼š{sale_time}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"è™•ç†æ´»å‹•å¤±æ•— {url}: {e}")
            return {
                'index': index,
                'title': "éŒ¯èª¤",
                'event_info': "æå–å¤±æ•—",
                'location': "æå–å¤±æ•—",
                'price': "æå–å¤±æ•—", 
                'sale_time': "æå–å¤±æ•—",
                'url': url
            }
            
    def scrape_all_events(self):
        """çˆ¬å–æ‰€æœ‰æ´»å‹•ä¸¦è™•ç† - å³æ™‚å„²å­˜ç‰ˆ"""
        output_file = 'tixcraft_activities.json'
        
        try:
            self.setup_driver()
            
            # ç²å–æ´»å‹•åˆ—è¡¨
            base_url = "https://tixcraft.com/activity"
            self.driver.get(base_url)
            
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.thumbnails"))
            )
            
            # æ”¶é›†æ‰€æœ‰æ´»å‹•é€£çµ
            activity_links = []
            thumbnails = self.driver.find_elements(By.CSS_SELECTOR, "div.thumbnails a")
            
            for link in thumbnails:
                href = link.get_attribute('href')
                if href and 'activity/detail' in href:
                    activity_links.append(href)
                    
            total_activities = len(activity_links)
            self.logger.info(f"ğŸ¯ æ‰¾åˆ° {total_activities} å€‹æ´»å‹•ï¼Œé–‹å§‹å³æ™‚è™•ç†...")
            
            # å³æ™‚è™•ç†æ¯å€‹æ´»å‹•
            current_success_count = 0
            
            for index, url in enumerate(activity_links, 1):
                # è™•ç†å–®ä¸€æ´»å‹•
                event_data = self.process_single_event(url, index)
                
                if event_data['title'] not in ["éŒ¯èª¤", "æå–å¤±æ•—"]:
                    current_success_count += 1
                
                # ===== å³æ™‚å„²å­˜æ¨¡å¼ï¼šæ¯è™•ç†å®Œä¸€å€‹æ´»å‹•å°±ç«‹åˆ»å­˜æª” =====
                try:
                    # è®€å–ç¾æœ‰è³‡æ–™ï¼ˆæ¯æ¬¡éƒ½é‡æ–°è¼‰å…¥ç¢ºä¿æœ€æ–°ï¼‰
                    existing_data = self.load_existing_data(output_file)
                    
                    # åˆä½µæ–°èˆŠè³‡æ–™ï¼ˆå°‡ç•¶å‰è™•ç†çš„æ´»å‹•åŠ å…¥ï¼‰
                    current_events = [event_data]  # ç•¶å‰åªæœ‰ä¸€å€‹æ´»å‹•
                    merged_events = self.merge_data(existing_data, current_events)
                    
                    # é‡æ–°è¨ˆç®—çµ±è¨ˆè³‡æ–™ - æ›´æ–°ç‚ºæ–°æ¬„ä½çµæ§‹
                    merged_success_count = sum(1 for event in merged_events 
                                             if not all(field == "æœªæ‰¾åˆ°" for field in [
                                                 event.get('event_info', ''),  # æ›´æ–°ç‚ºåˆä½µæ¬„ä½
                                                 event.get('location', ''),
                                                 event.get('price', ''),
                                                 event.get('sale_time', '')
                                             ]))
                    merged_success_rate = (merged_success_count / len(merged_events)) * 100 if merged_events else 0
                    
                    # æº–å‚™å³æ™‚çµæœ
                    real_time_result = {
                        'scrape_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'last_update': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'total_events': len(merged_events),
                        'success_count': merged_success_count,
                        'success_rate': f'{merged_success_rate:.1f}%',
                        'extraction_method': 'realtime_precision_field_extraction',
                        'current_progress': f'{index}/{total_activities}',
                        'current_session_success': current_success_count,
                        'events': merged_events
                    }
                    
                    # å³æ™‚å¯«å…¥ JSON æª”æ¡ˆ
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(real_time_result, f, ensure_ascii=False, indent=2)
                    
                    # å¢åŠ å®‰å…¨æ„Ÿï¼šæ¯æ¬¡å­˜æª”å¾Œåœ¨ Log ä¸­å°å‡ºé€²åº¦
                    self.logger.info(f"ğŸ’¾ å·²åŒæ­¥è‡³ JSON æª”æ¡ˆï¼Œç›®å‰é€²åº¦ï¼š{index}/{total_activities} ({index/total_activities*100:.1f}%)")
                    self.logger.info(f"ğŸ“Š æœ¬æ¬¡æˆåŠŸï¼š{current_success_count}ï¼Œæ•´é«”æˆåŠŸç‡ï¼š{merged_success_rate:.1f}%")
                    
                except Exception as save_error:
                    self.logger.error(f"âŒ å³æ™‚å­˜æª”å¤±æ•—: {save_error}")
                    # å³ä½¿å­˜æª”å¤±æ•—ä¹Ÿç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹æ´»å‹•
                
                # éš¨æ©Ÿç­‰å¾…æ™‚é–“ï¼Œæ¨¡æ“¬çœŸå¯¦ç€è¦½è¡Œç‚º
                time.sleep(random.uniform(2, 4))
                
                # æ¯ 5 å€‹æ´»å‹•å°±ä¼‘æ¯ 8-12 ç§’
                if index % 5 == 0:
                    rest_time = random.uniform(8, 12)
                    self.logger.info(f"â¸ï¸  å·²è™•ç† {index} å€‹æ´»å‹•ï¼Œä¼‘æ¯ {rest_time:.1f} ç§’ä»¥é¿å…è¢«å°é–...")
                    time.sleep(rest_time)
            
            # æœ€çµ‚çµ±è¨ˆå’Œç¢ºèª
            final_data = self.load_existing_data(output_file)
            final_events = final_data.get('events', []) if final_data else []
            
            self.logger.info(f"ğŸ‰ æ‰€æœ‰æ´»å‹•è™•ç†å®Œæˆï¼")
            self.logger.info(f"ğŸ“ˆ æœ¬æ¬¡å·¥ä½œéšæ®µæˆåŠŸè™•ç†ï¼š{current_success_count}/{total_activities} å€‹æ´»å‹•")
            self.logger.info(f"ğŸ“‚ æœ€çµ‚è³‡æ–™åº«åŒ…å«ï¼š{len(final_events)} å€‹æ´»å‹•")
            self.logger.info(f"ğŸ’¾ å³æ™‚å„²å­˜å®Œæˆï¼š{output_file}")
            
            # è¿”å›æœ€çµ‚çµæœ
            return final_data if final_data else {
                'total_events': 0,
                'success_count': 0,
                'success_rate': '0.0%',
                'current_session_success': current_success_count,
                'events': []
            }
            
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return None
        finally:
            if self.driver:
                self.driver.quit()
                
def main():
    scraper = TixcraftPrecisionFieldScraper()
    result = scraper.scrape_all_events()
    
    if result:
        print(f"\n=== ğŸ¯ å³æ™‚å„²å­˜ç‰ˆè³‡æ–™æ›´æ–°çµæœ ===")
        print(f"æ›´æ–°æ™‚é–“ï¼š{result.get('last_update', 'N/A')}")
        print(f"è™•ç†é€²åº¦ï¼š{result.get('current_progress', 'N/A')}")
        print(f"æœ¬æ¬¡å·¥ä½œéšæ®µæˆåŠŸï¼š{result.get('current_session_success', 0)} å€‹")
        print(f"è³‡æ–™åº«ç¸½è¨ˆï¼š{result.get('total_events', 0)} å€‹æ´»å‹•")
        print(f"æ•´é«”æˆåŠŸç‡ï¼š{result.get('success_rate', '0.0%')}")
        print(f"æå–æ–¹æ³•ï¼š{result.get('extraction_method', 'realtime_precision_field_extraction')}")
        print(f"å„²å­˜æª”æ¡ˆï¼štixcraft_activities.json")
        
        # é¡¯ç¤ºæœ€è¿‘æ›´æ–°çš„5å€‹æ´»å‹•ç¤ºä¾‹
        events = result.get('events', [])
        if events:
            print(f"\n=== ğŸ” æœ€æ–°æ´»å‹•è³‡æ–™ç¤ºä¾‹ ===")
            recent_events = events[:5]  # å–å‰5å€‹ä½œç‚ºç¤ºä¾‹
            for i, event in enumerate(recent_events, 1):
                print(f"\nã€æ´»å‹• {i}ã€‘{event.get('title', 'N/A')}")
                print(f"  ğŸ“… æ¼”å‡ºè³‡è¨Šï¼š{event.get('event_info', 'N/A')}")  # åˆä½µé¡¯ç¤º
                print(f"  ğŸ“ åœ°é»ï¼š{event.get('location', 'N/A')}")
                print(f"  ğŸ’° ç¥¨åƒ¹ï¼š{event.get('price', 'N/A')}")
                print(f"  ğŸŸï¸ å”®ç¥¨æ™‚é–“ï¼š{event.get('sale_time', 'N/A')}")
        
        print(f"\nâœ… å³æ™‚å„²å­˜åŠŸèƒ½å·²å•Ÿç”¨ï¼")
        print(f"ğŸ’¾ æ¯è™•ç†ä¸€å€‹æ´»å‹•éƒ½æœƒç«‹åˆ»æ›´æ–° JSON æª”æ¡ˆ")
        print(f"ğŸ”„ è³‡æ–™å³æ™‚åŒæ­¥ï¼Œç„¡éœ€ç­‰å¾…å…¨éƒ¨å®Œæˆ")
        print(f"ğŸ“Š ç¸½å…±ç®¡ç† {result.get('total_events', 0)} å€‹æ´»å‹•è³‡æ–™")
        
    else:
        print("âŒ çˆ¬å–å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒæ–‡ä»¶")

if __name__ == "__main__":
    main()